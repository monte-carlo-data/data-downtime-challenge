{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Downtime Challenge | Exercise 1\n",
    "\n",
    "## 0. Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'utils'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-06183d3cb3bd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mutils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mall_days\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshow_reports\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'utils'"
     ]
    }
   ],
   "source": [
    "from ..utils import all_days, show_reports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "\n",
    "conn = sqlite3.connect('Ex1.db')\n",
    "c = conn.cursor()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Introduction\n",
    "\n",
    "Welcome to the Data Downtime Challenge! In this exercise, we'll learn Data Observability through practice on some sample datasets. Each subproblem will ask you to craft some `SQL` queries that help us learn about the state of our tables and identify Data Downtime issues.\n",
    "\n",
    "For these exercises, we'll be using mock astronomical data to identify habitable planets.\n",
    "\n",
    "![SegmentLocal](/tree/assets/planets.gif \"segment\")\n",
    "\n",
    "The `Ex1.db` database contains a single table, `EXOPLANETS`, which has information on nearly 2000 exoplanets across the Milky Way Galaxy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c.execute(\"PRAGMA table_info(EXOPLANETS);\")\n",
    "c.fetchall()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A database entry in `EXOPLANETS` contains the following info:\n",
    "\n",
    "0. `_id`: A UUID corresponding to the planet.\n",
    "1. `distance`: Distance from Earth, in lightyears.\n",
    "2. `g`: Surface gravity as a multiple of $g$, the gravitational force constant.\n",
    "3. `orbital_period`: Length of a single orbital cycle in days.\n",
    "4. `avg_temp`: Average surface temperature in degrees Kelvin.\n",
    "5. `date_added`: The date our system discovered the planet and added it automatically to our databases.\n",
    "\n",
    "Note that one or more of `distance`, `g`, `orbital_period`, and `avg_temp` may be `NULL` for a given planet as a result of missing or erroneous data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.read_sql_query(\"SELECT * FROM EXOPLANETS LIMIT 10\", conn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Exercise: Visualizing Freshness"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Grouping by the `DATE_ADDED` column can give us insight into how `EXOPLANETS` updates daily. For example, we can query for the number of new IDs added per day:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SQL = \"\"\"\n",
    "SELECT\n",
    "    DATE_ADDED,\n",
    "    COUNT(*) AS ROWS_ADDED\n",
    "FROM\n",
    "    EXOPLANETS\n",
    "GROUP BY\n",
    "    DATE_ADDED\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rows_added = pd.read_sql_query(SQL, conn)\n",
    "rows_added = rows_added \\\n",
    "    .rename(columns={clmn: clmn.lower() for clmn in rows_added.columns})\n",
    "rows_added = rows_added.set_index(\"date_added\")\n",
    "rows_added = rows_added.reindex(all_days)\n",
    "plt.figure(figsize=(20, 10))\n",
    "plt.bar(all_days, height=rows_added[\"rows_added\"])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks like `EXOPLANETS` typically updates with around 100 new entries each day. Something looks off in a few places, though. We have what we'd call a **freshness** incident -- on a couple of occasions, the table doesn't update at all for a 3 or more days. It has \"stale\" (3+ day old) data.\n",
    "\n",
    "Try writing some `SQL` code that returns timestamps for when freshness incidents occur. Feel free to use the query above as a starting point.\n",
    "\n",
    "- *Hint 1*: The `LAG` window function should help in comparing two subsequent rows in a query.\n",
    "- *Hint 2*: `SQLite` uses `JULIANDAY()` to cast an object to a date.\n",
    "- *Hint 3*: An example solution is given in `exercise_1_soln.ipynb` in this same directory, if needed for comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "SQL = \"\"\"\n",
    "\n",
    "\"\"\"\n",
    "# END YOUR CODE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "freshness_anoms = pd.read_sql_query(SQL, conn)\n",
    "freshness_anoms = freshness_anoms \\\n",
    "    .rename(columns={clmn: clmn.lower() for clmn in freshness_anoms.columns})\n",
    "freshness_anoms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `show_reports` function from `utils` will help us visualize our results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_reports(rows_added, freshness_anoms[\"date_added\"], \"rows_added\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Exercise: Null Rates\n",
    "\n",
    "We can also look at the rate of new `NULL` entries for different columns on a daily basis:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SQL = \"\"\"\n",
    "SELECT\n",
    "    DATE_ADDED,\n",
    "    CAST(SUM(CASE WHEN DISTANCE IS NULL THEN 1 ELSE 0 END) AS FLOAT) / COUNT(*) AS DISTANCE_NULL_RATE,\n",
    "    CAST(SUM(CASE WHEN G IS NULL THEN 1 ELSE 0 END) AS FLOAT) / COUNT(*) AS G_NULL_RATE,\n",
    "    CAST(SUM(CASE WHEN ORBITAL_PERIOD IS NULL THEN 1 ELSE 0 END) AS FLOAT) / COUNT(*) AS ORBITAL_PERIOD_NULL_RATE,\n",
    "    CAST(SUM(CASE WHEN AVG_TEMP IS NULL THEN 1 ELSE 0 END) AS FLOAT) / COUNT(*) AS AVG_TEMP_NULL_RATE    \n",
    "FROM\n",
    "    EXOPLANETS\n",
    "GROUP BY\n",
    "    DATE_ADDED\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "null_rates = pd.read_sql_query(SQL, conn)\n",
    "null_rates = null_rates \\\n",
    "    .rename(columns={clmn: clmn.lower() for clmn in null_rates.columns})\n",
    "null_rates = null_rates.set_index(\"date_added\")\n",
    "null_rates = null_rates.reindex(all_days)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20, 10))\n",
    "plt.bar(all_days, height=null_rates[\"distance_null_rate\"])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20, 10))\n",
    "plt.bar(all_days, height=null_rates[\"g_null_rate\"])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20, 10))\n",
    "plt.bar(all_days, height=null_rates[\"orbital_period_null_rate\"])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20, 10))\n",
    "plt.bar(all_days, height=null_rates[\"avg_temp_null_rate\"])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some of the fields in the table show \"spikes\" in the rate of null values at certain times. Try to write a simple `SQL` query to identify timestamps with unusually high null rates. Again, feel free to use the query above as a starting point.\n",
    "\n",
    "- *Hint 1*: It may help to introduce a column that identifies which field (or fields) have the anomalous null rate for a given timestamp.\n",
    "- *Hint 2*: Once again, a sample solution is available in `ex1_soln.ipynb`, if needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "SQL = \"\"\"\n",
    "\n",
    "\"\"\"\n",
    "# END YOUR CODE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "null_rate_anoms = pd.read_sql_query(SQL, conn)\n",
    "null_rate_anoms = null_rate_anoms \\\n",
    "    .rename(columns={clmn: clmn.lower() for clmn in null_rate_anoms.columns})\n",
    "null_rate_anoms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once again, we'll use the `show_reports` function to visualize the results of our query. Feel free to continue running the above and below cells until you're happy with the reports!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_reports(null_rates, null_rate_anoms[\"date_added\"], \"avg_temp_null_rate\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great work! In the next exercise, we'll build off of these simpler reports to handle scenarios with multiple tables and lineage information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
